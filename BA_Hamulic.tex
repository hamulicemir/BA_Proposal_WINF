\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=ieee,citestyle=numeric]{biblatex} % For citations
\usepackage{setspace}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{comment}
\usepackage{tabularx}
\usepackage{array} % für bessere Spaltentypen
\usepackage{booktabs}
\usepackage{pgfgantt}
\usepackage{graphicx}
\usepackage{float}



% Page setup
\geometry{margin=1in}
\setstretch{1.2}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.5em}{}

% Bibliography
\addbibresource{bibliography.bib}

% --- Meta-Daten ---
\newcommand{\studentid}{wi23b168}
\newcommand{\studyprogram}{B.Sc. Business Informatics}
\newcommand{\advisor}{Rohatsch Lukas, MSc}
\newcommand{\university}{University of Applied Sciences Technikum Wien}
\newcommand{\version}{0.1}


% Title info
\title{\textbf{Proposal Bachelor's Thesis}\\[0.5em]
Development and Evaluation of a Hybrid Approach for Automated Error Detection and Classification in LoRaWAN-Based IoT Data Pipelines Using Log Data}
\author{Emir Hamulic}
\date{
\university\\[1em]
\small
\textbf{Student ID:} \studentid\\
\textbf{Study Program:} \studyprogram\\
\textbf{Advisor:} \advisor\\
\textbf{Version:} \version\\[0.5em]
\textbf{Date:} \today
}

\begin{document}

\maketitle

\section{Problem Area}

\subsection{Operational Context}
LoRaWAN-based IoT deployments are usually operated as distributed, multi-component systems.
End devices communicate with gateways, which forward packets to a LoRaWAN Network Server (LNS). The LNS then routes data to application components or ETL-Pipelines which process data into structured data for databases or dashboards.
This “star-of-stars” topology and the central role of the Network Server are core characteristics of LoRaWAN networks. \cite{LoRaAllianceLoRaWANv11}

\subsection{Log Volume and Monitoring Limit}
Modern IT and IoT backends generate very large volumes of logs. Manual inspection becomes impractical at scale therefore log analysis and diagnosis must be automated. \cite{Oliner2012LogAnalysis}

Practitioner evidence reinforces this: in a large survey on log anomaly detection, practitioners explicitly cite the need to “efficiently analyze large volumes of log data,”
and many expect tools to handle at least 100,000 logs while still delivering near-real-time results. \cite{Ma2024PractitionersLogAD}

\subsection{Heterogeneity and Missing Structure}
Operational logs are produced by numerous components in an interleaved fashion, are highly heterogeneous and are semi-structured.
Automated log analysis typically begins by extracting templates that convert raw, unstructured log messages into a structured event representation.
Research further demonstrates that diverse formatting and log types create significant challenges when applying maschine learning methods directly to real-world logs, and that preprocessing decisions materially impact analysis quality.\cite{He2017Drain}

\subsection{Missing Prioritization}
Raw logs usually do not provide a prioritization scheme or support for decisions like confidence estimates, rationale, or action guidance.
This forces operators to manually inspect and link alerts, which is driving up the MTTR and operational cost. \cite{Ma2024PractitionersLogAD}

\subsection{Rule-Based Detection Systems}
Rule-based systems are effective for known patterns, but they require effort and are limited when log messages are variable or completely new patterns appear.
Initial approaches to log anomaly detection relied mainly on manually defined rules. More recent studies favor deep neural networks that model normal log behavior and identify deviations.\cite{Landauer2023LogADSurvey}

\subsection{ML-Based Detection}
The ML component is primarily used as a mechanism to detect and prioritize \emph{unknown} events.
Anomaly detection methods identify outliers for operational attention by learning a baseline of typical log behavior rather than requiring an explicit signature for every failure. \cite{Landauer2023LogADSurvey,Du2017DeepLog}
This is relevant for long-tail failures such as new stack traces, unexpected integration errors, or previously unseen combinations of events.\cite{Ma2024PractitionersLogAD}

A Random Forest is a comparatively lightweight classifier for tabular log features.
It is suitable for classification without the complexity of deep models due to the aggregation of many decision trees, which can output estimates for class probabilities.\cite{Breiman2001RandomForests}
In a hybrid pipeline, these probability estimates can be used to implement a reject option for unknown events. When the maximum predicted class probability falls below a threshold, the event is tagged
as \textit{Unknown} and routed to aggregation and sometimes with rule-base enrichment. \cite{Breiman2001RandomForests,Vaze2022OSRClosedSet}

With an open-set recognition strategy, the Machine Learning subsystem should not force every input to an error class.
Inputs from unseen classes must be rejected or mapped to an ``unknown'' category based on confidence or risk-aware decision rules. \cite{Scheirer2014OpenSet}

\newpage


\section{Task Description}
The main aim of this thesis is to design and build a prototype system that automatically analyzes, classifies, and temporally aggregates LoRaWAN pipeline log data.
The prototype system is implemented in Python. MariaDB is used primarily as a persistence layer for events to enable queries for monitoring and reproducible evaluation.
The aim of the prototype is to conduct a controlled comparison between three strategies using empirical metrics, under consideration of both classification quality and operational advantages.

\subsection{Objective}
Manual log monitoring does not scale for high-volume and heterogeneous LoRaWAN pipeline logs and provides limited prioritization for incident events.
Rule-based detection is precise for known signatures but struggles with message variability and previously unseen events.
This work therefore evaluates whether an ML component and a rules-first hybrid strategy is able to improve automated classification and decision support compared to a rule-only method.

\subsection{Target Audience}
The target audience are technical stakeholders: IoT/backend engineers, applied Data/ML Engineers and operations/SRE roles, working with LoRaWAN data pipelines.
The intended use is to provide a reproducible prototype workflow for structured and aggregated error views, and empirical evidence to select an appropriate detection strategy for specific error classes and operational requirements.

\subsection{Expected Outcomes}
The outcome of this thesis should be that readers are able to reproduce:
(i) how to structure LoRaWAN pipeline logs into machine-processable events,
(ii) how to implement a rule-based baseline,
(iii) how to train and apply an ML classifier with confidence-based handling of \textit{Unknown} events.

\subsection{Work Steps}
The work is organized into the following technical steps:
\begin{enumerate}
    \item \textbf{Parsing and Structuring:} Convertation of raw log-lines into structured events for deterministic matching, Machine Learning features, and aggregation.
    \item \textbf{Labeling:} Definition of an operational error taxonomy (classes, examples, priorities) and a labeling approach for Machine Learning and evaluation.
    \item \textbf{Rule-Based Classifier:} Implementation of a deterministic detector that assigns classes and priorities for explicit, stable signatures. \textit{Unknown} events should be routed to a residual category.
    \item \textbf{Machine Learning-Based Classifier:} Implementation of an Machine Learning-based classifier with engineered features and calculated confidence estimates.
    \item \textbf{Hybrid Classifier:} Implementation of a rule-based Classifier first and a Machine Learning-based classifier classifying based of confidence for \textit{Unknown} events.
    \item \textbf{Aggregation and Storage:} Aggregatation of events over time windows and storing summaries like confidence statistics or affected entities inside MariaDB.
    \item \textbf{Evaluation:} Definition of a test protocol with two scenarios:
          \begin{itemize}
              \item \textit{Known-classes test (closed-set):} Evaluation of how well rule-based, ML-based, and hybrid approaches classify log events into the predefined error classes.
              \item \textit{Novelty test (open-set):} Creation of a ``unknown'' test set by holding out classes or log templates during training. Evaluation of whether the approaches correctly assign to \textit{Unknown} while still classifying the remaining known cases.
          \end{itemize}

    \item \textbf{Empirical Comparison and Reporting:} Comparison of rule-based, ML-based, and hybrid approaches with metrics at two levels:
          \begin{enumerate}
              \item log-line level with precision/recall/F1 and confusion analysis
              \item time-window/incident level with quality of aggregated summaries
          \end{enumerate}
          The novelty test reports \textit{Unknown-Recall} and \textit{False-Reject Rate}. The results are summarized and could be used for guidance on when rules, ML, or a hybrid approach is most suitable.
\end{enumerate}

\newpage


\section{Research Questions / Hypotheses}

\textbf{Main Research Question (MRQ):}\\
How effective is a hybrid approach for automated error detection, classification and aggregation in LoRaWAN-based IoT data pipelines?
\vspace{1em}

\noindent\textbf{Sub Research Question 1 (SRQ1):}\\
Which practically relevant error and warning events can be identified from real LoRaWAN pipeline logs and formalized into a consistent class catalogue?

\vspace{1em}
\noindent\textbf{Sub Research Question 2 (SRQ2):}\\
How do rule-based, Machine Learning-based, and hybrid approaches differ in measurable performance at log-line and time-window/incident level?

\vspace{1em}
\noindent\textbf{Sub Research Question 3 (SRQ3):}\\
Which error classes are better addressed by deterministic rules, which benefit more from Machine Learning–based classification, and which characteristics explain these differences?

%\vspace{0.5em}
%\textbf{Hypothesis (H1):}\\
%Compared to a rule-only baseline, a rules-first hybrid approach achieves higher overall utility (e.g., macro-averaged F1 at log-line level and/or improved window/incident-level metrics), while maintaining comparable precision for classes with clear and stable signatures.

\newpage
\section{Methodology}

\subsection{Research Design}
This thesis follows an engineering-oriented research design combining \textit{prototyping} with \textit{quantitative data analysis} and \textit{literature reviews}.
The core artifact is a Python-based prototype that transforms raw LoRaWAN pipeline logs into structured, classified, and temporally aggregated events.
The artifact is evaluated empirically and compared across three strategies.

\subsection{Methods per Research Question}

\subsubsection{SRQ1: Error and Warning Classes}
\textbf{Methods: Qualitative content analysis and descriptive statistics.}\newline
Log messages are reviewed to derive error and warning categories using inductive coding.
The class list is consolidated with explicit definitions, class boundaries, and priorities.
Descriptive statistics check the occurrence of classes across components and time.\newline
\textbf{Suitability:}
SRQ1 builds a taxonomy from logs and checks the occurrence of classes to enable automation.

\subsubsection{SRQ2: Performance Comparison}
\textbf{Methods: Controlled comparative evaluation and quantitative measurement.}\newline
The three detection strategies are evaluated under a fixed protocol with the same dataset, splits, and metrics.
The novelty test is performed in the open-set setting, where a set of classes or log templates is held out in the training stage, and the methods are evaluated based on how well they are able to classify the held-out samples into the \textit{Unknown} class while maintaining a strong classification performance on the known classes.\cite{Scheirer2014OpenSet}
Results are reported at log-line and time-window/incident level.\newline
\textbf{Suitability:}
SRQ2 requires objective, comparable results. A controlled setup ensures that any performance differences are due to the detection strategy, not to changes in the data, splits, or evaluation procedure.

\subsubsection{SRQ3: Suitability of Rules and Machine Learning by Error Class}
\textbf{Methods: Class analysis and interpretation.}\newline
SRQ2 results are analyzed per error class and related to class characteristics such as stability, variability and rarity.
This supports assessing whether rules, Machine Learning, or a hybrid approach fits best per class.\newline
\textbf{Suitability:}
SRQ3 takes empirical results to guidance because it is linking the strategy performance to the strategy characteristics.

\newpage
\section{Kind of expected results}

\subsection{Expected Project Outputs}
Beyond the Research Question results, the thesis is expected to deliver these outputs:
\begin{itemize}
    \item A technical introduction to LoRaWAN and the observed IoT data pipeline context.
    \item A technical overview of ETL pipelines and the advantages of logging.
    \item An analysis of real LoRaWAN log data from an enterprise setting.
    \item A reproducible Python prototype for automated log classification and temporal aggregation, storing the events and summaries in MariaDB.
    \item Structured error overview replacing raw logs.
\end{itemize}

\subsection{Expected Results of the Research Questions}
The expected results of this thesis are structured by research question and follow directly from the selected methods and the developed prototype artifact.
\newline\newline
For \textbf{SRQ1}, the qualitative content analysis of real LoRaWAN pipeline logs is expected to produce a class catalogue that is suitable for automated processing.
The catalogue will define a bounded set of error and warning classes with meaningful priorities and clear class definitions.
Descriptive statistics are expected to establish a baseline of the observed log data, such as class frequencies and distributions over time and across components.
\newline\newline
For \textbf{SRQ2}, a controlled comparative assessment is expected to provide a reproducible performance comparison between all approaches.
This includes a standardized protocol, metric results at the log-line level, and monitoring oriented results at time-window or incident level.
The outcome will provide measurable evidence of performance differences, including how confidence-based handling of \textit{Unknown} events affects both novelty detection and the whole quality for known classes.
\newline\newline
For \textbf{SRQ3}, the class analysis produces a mapping that explains which error classes are most effectively handled by the three strategies.
The result will relate observed performance patterns to class characteristics such as stability, variability, context and rarity.
This is expected to offer practiceable guidance on how to prioritize rule development, where ML adds operational value, or how this could be used for \textit{Unknown} cases as input for iterative refinement of the class catalogue and the rule base.

\newpage

\subsection{Overview: Research Questions, Methods, and Expected Result Types}

\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}X
            >{\raggedright\arraybackslash}X
            >{\raggedright\arraybackslash}X}
        \toprule
        \textbf{Research questions / hypothesis}                                                                                                                                              &
        \textbf{Method(s)}                                                                                                                                                                    &
        \textbf{Expected kind of result}                                                                                                                                                        \\
        \midrule
        SRQ1: Which practically relevant error and warning classes can be derived from real LoRaWAN pipeline logs and formalized into a consistent class catalogue?                           &
        Qualitative content analysis; descriptive statistics                                                                                                                                  &
        A class catalogue with clear definitions, boundaries and priorities including representative log patterns.                                                                              \\
        \midrule
        SRQ2: How do rule-based, ML-based, and hybrid approaches differ in measurable performance at log-line and time-window/incident level?                                                 &
        Controlled comparative evaluation; quantitative performance measurement                                                                                                               &
        An empirical comparison report including per-class metrics on log-line level and monitoring-oriented evaluation on time-window/incident level for all three approaches.                 \\
        \midrule
        SRQ3: Which error classes are better addressed by deterministic rules and which benefit more from ML-based classification, and which class characteristics explain these differences? &
        Per-class analysis; deductive interpretation                                                                                                                                          &
        An evidence-based suitability mapping of error classes to detection strategy, with explanations based on observable class characteristics.                                              \\
        %\midrule
        %H1: Compared to a rule-only baseline, a rules-first hybrid approach achieves higher overall utility while maintaining comparable precision for stable-signature classes.              &
        %$Hypothesis testing via SRQ2 metrics and SRQ3 interpretation                                                                                                                           &
        %A quantified assessment of hypothesis support, including identified trade-offs such as improvements in overall utility and potential impacts on precision for signature-based classes.  \\
        \bottomrule
    \end{tabularx}
    \caption{Mapping research questions to methods and expected result types.}
\end{table}



\newpage

\section{Timetable}

\subsection{Milestone Plan}
\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{6pt}
    \begin{tabularx}{\textwidth}{
        >{\raggedright\arraybackslash}p{0.5cm}
        >{\raggedright\arraybackslash}p{6.8cm}
        >{\raggedright\arraybackslash}p{2.5cm}
        >{\raggedright\arraybackslash}p{2cm}
        >{\raggedright\arraybackslash}p{2cm}
        }
        \toprule
        \textbf{Nr}         &
        \textbf{Name}       &
        \textbf{Plan}       &
        \textbf{Adapted by} &
        \textbf{Actual Date}                                                             \\
        \midrule
        M1                  & Proposal approved                   & 31.01.2026 & -- & -- \\
        M2                  & Literature review completed         & 02.03.2026 & -- & -- \\
        M3                  & Data preparation completed          & 12.03.2026 & -- & -- \\
        M4                  & PoC implementation completed        & 01.04.2026 & -- & -- \\
        M5                  & Partial submission and presentation & 10.04.2026 & -- & -- \\
        M6                  & Complete first version handed in    & 01.05.2026 & -- & -- \\
        M7                  & Final thesis version handed in      & 22.05.2026 & -- & -- \\
        \bottomrule
    \end{tabularx}
    \caption{Milestone plan vs.\ actuals.}
\end{table}


\subsection{Project Plan (Gantt Chart)}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/gantt-chart.png}
    \caption{Project plan (Gantt chart).}
\end{figure}
\newpage

\newpage
\clearpage

\listoftables
\vspace{1em}
\listoffigures


\newpage


\newpage


\printbibliography
\end{document}
